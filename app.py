import streamlit as st
import os
import torch
import ffmpeg
import tempfile
from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Transcri√ß√£o de √Åudio com Whisper",
    page_icon="üéôÔ∏è",
    layout="centered"
)

# --- Cache do Modelo ---
# O cache do Streamlit evita recarregar o modelo pesado toda vez.
@st.cache_resource
def load_model():
    """Carrega e armazena em cache o modelo e o pipeline do Whisper."""
    # Mudar para um modelo maior se estiver usando uma GPU no Hugging Face Spaces
    # model_id = "openai/whisper-large-v3" 
    model_id = "openai/whisper-small" # √ìtimo para come√ßar com CPU

    try:
        model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)
        processor = AutoProcessor.from_pretrained(model_id)
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            device="cpu",  # For√ßar CPU para o n√≠vel gratuito do HF Spaces
            torch_dtype=torch.float32,
        )
        st.success("Modelo de IA carregado com sucesso!")
        return pipe
    except Exception as e:
        st.error(f"Erro ao carregar o modelo: {e}")
        return None

# --- Interface do Usu√°rio ---
st.title("üéôÔ∏è Transcri√ß√£o de √Åudio com Whisper")
st.markdown("Fa√ßa o upload de um arquivo de √°udio (MP3, WAV, M4A) e obtenha a transcri√ß√£o em segundos.")

# Carrega o pipeline de IA
transcription_pipe = load_model()

if transcription_pipe:
    uploaded_file = st.file_uploader(
        "Escolha um arquivo de √°udio", 
        type=['mp3', 'wav', 'm4a', 'ogg', 'flac']
    )

    if uploaded_file is not None:
        st.audio(uploaded_file, format=uploaded_file.type)
        
        if st.button("Transcrever √Åudio Agora"):
            with st.spinner("O processo de transcri√ß√£o foi iniciado. Por favor, aguarde..."):
                try:
                    # Salva o arquivo em um local tempor√°rio
                    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        temp_audio_path = tmp_file.name
                    
                    # Executa a transcri√ß√£o
                    result = transcription_pipe(
                        temp_audio_path,
                        chunk_length_s=30, # Processa em peda√ßos de 30s
                        batch_size=4,
                        return_timestamps=True
                    )
                    transcription_text = result["text"]
                    
                    st.divider()
                    st.subheader("‚úÖ Transcri√ß√£o Conclu√≠da:")
                    st.text_area("Resultado", transcription_text, height=250)
                    
                    # Bot√£o de download
                    st.download_button(
                        label="Baixar Transcri√ß√£o (.txt)",
                        data=transcription_text.encode("utf-8"),
                        file_name=f"{os.path.splitext(uploaded_file.name)[0]}_transcricao.txt",
                        mime="text/plain"
                    )

                except Exception as e:
                    st.error(f"Ocorreu um erro durante a transcri√ß√£o: {e}")
                finally:
                    # Garante que o arquivo tempor√°rio seja removido
                    if 'temp_audio_path' in locals() and os.path.exists(temp_audio_path):
                        os.remove(temp_audio_path)
else:
    st.warning("O pipeline de transcri√ß√£o n√£o p√¥de ser carregado. A aplica√ß√£o n√£o pode funcionar.")